#!/usr/bin/env python3
"""
upload_to_bigquery.py - Upload dá»¯ liá»‡u BHYT tá»« Excel lÃªn BigQuery
===================================================================
Sá»­ dá»¥ng: source venv/bin/activate && python upload_to_bigquery.py CPBQ.xlsx

TÃ­nh nÄƒng:
  - Äá»c file Excel, chuáº©n hÃ³a kiá»ƒu dá»¯ liá»‡u
  - Tá»± Ä‘á»™ng táº¡o dataset/table náº¿u chÆ°a cÃ³
  - Check trÃ¹ng láº·p row-level theo (ma_cskcb + ma_bn + ma_loaikcb + ngay_vao + ngay_ra)
  - ThÃªm metadata: upload_timestamp, source_file
"""

import sys
import os
from datetime import datetime

import pandas as pd
from google.cloud import bigquery
from google.api_core.exceptions import NotFound

from config import PROJECT_ID, DATASET_ID, TABLE_ID, FULL_TABLE_ID, LOCATION, SHEET_NAME
from auth import get_credentials


# â”€â”€â”€ BigQuery Schema â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

SCHEMA = [
    bigquery.SchemaField("stt", "INT64"),
    bigquery.SchemaField("ma_bn", "STRING"),
    bigquery.SchemaField("ho_ten", "STRING"),
    bigquery.SchemaField("ngay_sinh", "DATE"),
    bigquery.SchemaField("gioi_tinh", "INT64"),
    bigquery.SchemaField("dia_chi", "STRING"),
    bigquery.SchemaField("ma_the", "STRING"),
    bigquery.SchemaField("ma_dkbd", "STRING"),
    bigquery.SchemaField("gt_the_tu", "DATE"),
    bigquery.SchemaField("gt_the_den", "DATE"),
    bigquery.SchemaField("ma_benh", "STRING"),
    bigquery.SchemaField("ma_benhkhac", "STRING"),
    bigquery.SchemaField("ma_lydo_vvien", "INT64"),
    bigquery.SchemaField("ma_noi_chuyen", "STRING"),
    bigquery.SchemaField("ngay_vao", "DATETIME"),
    bigquery.SchemaField("ngay_ra", "DATETIME"),
    bigquery.SchemaField("so_ngay_dtri", "INT64"),
    bigquery.SchemaField("ket_qua_dtri", "INT64"),
    bigquery.SchemaField("tinh_trang_rv", "INT64"),
    bigquery.SchemaField("t_tongchi", "FLOAT64"),
    bigquery.SchemaField("t_xn", "FLOAT64"),
    bigquery.SchemaField("t_cdha", "FLOAT64"),
    bigquery.SchemaField("t_thuoc", "FLOAT64"),
    bigquery.SchemaField("t_mau", "FLOAT64"),
    bigquery.SchemaField("t_pttt", "FLOAT64"),
    bigquery.SchemaField("t_vtyt", "FLOAT64"),
    bigquery.SchemaField("t_dvkt_tyle", "FLOAT64"),
    bigquery.SchemaField("t_thuoc_tyle", "FLOAT64"),
    bigquery.SchemaField("t_vtyt_tyle", "FLOAT64"),
    bigquery.SchemaField("t_kham", "FLOAT64"),
    bigquery.SchemaField("t_giuong", "FLOAT64"),
    bigquery.SchemaField("t_vchuyen", "FLOAT64"),
    bigquery.SchemaField("t_bntt", "FLOAT64"),
    bigquery.SchemaField("t_bhtt", "FLOAT64"),
    bigquery.SchemaField("t_ngoaids", "FLOAT64"),
    bigquery.SchemaField("ma_khoa", "STRING"),
    bigquery.SchemaField("nam_qt", "INT64"),
    bigquery.SchemaField("thang_qt", "INT64"),
    bigquery.SchemaField("ma_khuvuc", "STRING"),
    bigquery.SchemaField("ma_loaikcb", "INT64"),
    bigquery.SchemaField("ma_cskcb", "STRING"),
    bigquery.SchemaField("noi_ttoan", "INT64"),
    bigquery.SchemaField("giam_dinh", "STRING"),
    bigquery.SchemaField("t_xuattoan", "FLOAT64"),
    bigquery.SchemaField("t_nguonkhac", "FLOAT64"),
    bigquery.SchemaField("t_datuyen", "FLOAT64"),
    bigquery.SchemaField("t_vuottran", "FLOAT64"),
    # Metadata columns
    bigquery.SchemaField("upload_timestamp", "TIMESTAMP"),
    bigquery.SchemaField("source_file", "STRING"),
]

# Composite key xÃ¡c Ä‘á»‹nh 1 Ä‘á»£t Ä‘iá»u trá»‹ duy nháº¥t cá»§a bá»‡nh nhÃ¢n
ROW_KEY_COLS = ["ma_cskcb", "ma_bn", "ma_loaikcb", "ngay_vao", "ngay_ra"]


# â”€â”€â”€ Data Transformation â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def parse_date_int(val):
    """Chuyá»ƒn int YYYYMMDD â†’ datetime.date, tráº£ None náº¿u lá»—i."""
    if pd.isna(val):
        return None
    try:
        s = str(int(val))
        return datetime.strptime(s, "%Y%m%d").date()
    except (ValueError, TypeError):
        return None


def parse_datetime_str(val):
    """Chuyá»ƒn string '202601020735' â†’ datetime, tráº£ None náº¿u lá»—i."""
    if pd.isna(val):
        return None
    try:
        s = str(val).strip().lstrip("'")
        if len(s) == 12:
            return datetime.strptime(s, "%Y%m%d%H%M")
        elif len(s) == 14:
            return datetime.strptime(s, "%Y%m%d%H%M%S")
        elif len(s) == 8:
            return datetime.strptime(s, "%Y%m%d")
        return None
    except (ValueError, TypeError):
        return None


def transform_dataframe(df: pd.DataFrame, source_filename: str) -> pd.DataFrame:
    """Chuáº©n hÃ³a kiá»ƒu dá»¯ liá»‡u cho táº¥t cáº£ cÃ¡c cá»™t."""
    print("  â³ Chuáº©n hÃ³a dá»¯ liá»‡u...")

    # Lowercase all column names
    df.columns = [c.lower().strip() for c in df.columns]

    # Date columns: YYYYMMDD int â†’ date
    for col in ["ngay_sinh", "gt_the_tu", "gt_the_den"]:
        if col in df.columns:
            df[col] = df[col].apply(parse_date_int)

    # Datetime columns: string â†’ datetime
    for col in ["ngay_vao", "ngay_ra"]:
        if col in df.columns:
            df[col] = df[col].apply(parse_datetime_str)

    # String columns: ensure str type
    str_cols = ["ma_bn", "ma_the", "ma_dkbd", "ma_benh", "ma_benhkhac",
                "ma_noi_chuyen", "ma_khoa", "ma_khuvuc", "ma_cskcb",
                "giam_dinh", "ho_ten", "dia_chi"]
    for col in str_cols:
        if col in df.columns:
            df[col] = df[col].apply(lambda x: str(x) if pd.notna(x) and x != "" else None)
            # Clean 'nan' strings
            df[col] = df[col].replace("nan", None)

    # Float columns: ensure float type
    float_cols = ["t_tongchi", "t_xn", "t_cdha", "t_thuoc", "t_mau",
                  "t_pttt", "t_vtyt", "t_dvkt_tyle", "t_thuoc_tyle",
                  "t_vtyt_tyle", "t_kham", "t_giuong", "t_vchuyen",
                  "t_bntt", "t_bhtt", "t_ngoaids", "t_xuattoan",
                  "t_nguonkhac", "t_datuyen", "t_vuottran"]
    for col in float_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")

    # Int columns: coerce to int
    int_cols = ["stt", "gioi_tinh", "ma_lydo_vvien", "so_ngay_dtri",
                "ket_qua_dtri", "tinh_trang_rv", "nam_qt", "thang_qt",
                "ma_loaikcb", "noi_ttoan"]
    for col in int_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors="coerce")

    # Add metadata columns
    df["upload_timestamp"] = datetime.utcnow()
    df["source_file"] = source_filename

    print(f"  âœ… Chuáº©n hÃ³a xong: {len(df)} dÃ²ng")
    return df


# â”€â”€â”€ BigQuery Operations â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def ensure_dataset(client: bigquery.Client):
    """Táº¡o dataset náº¿u chÆ°a tá»“n táº¡i."""
    dataset_ref = f"{PROJECT_ID}.{DATASET_ID}"
    try:
        client.get_dataset(dataset_ref)
        print(f"  âœ… Dataset '{DATASET_ID}' Ä‘Ã£ tá»“n táº¡i")
    except NotFound:
        dataset = bigquery.Dataset(dataset_ref)
        dataset.location = LOCATION
        dataset.description = "Dá»¯ liá»‡u chi phÃ­ báº£o quáº£n BHYT - TTYT Thá»§y NguyÃªn"
        client.create_dataset(dataset)
        print(f"  âœ… ÄÃ£ táº¡o dataset '{DATASET_ID}' táº¡i {LOCATION}")


def ensure_table(client: bigquery.Client):
    """Táº¡o table náº¿u chÆ°a tá»“n táº¡i."""
    try:
        client.get_table(FULL_TABLE_ID)
        print(f"  âœ… Table '{TABLE_ID}' Ä‘Ã£ tá»“n táº¡i")
    except NotFound:
        table = bigquery.Table(FULL_TABLE_ID, schema=SCHEMA)
        table.description = "Dá»¯ liá»‡u thanh toÃ¡n BHYT hÃ ng thÃ¡ng"
        # Partition by thang_qt for efficient querying
        table.range_partitioning = bigquery.RangePartitioning(
            field="thang_qt",
            range_=bigquery.PartitionRange(start=1, end=13, interval=1),
        )
        table.clustering_fields = ["ma_cskcb", "ma_bn"]
        client.create_table(table)
        print(f"  âœ… ÄÃ£ táº¡o table '{TABLE_ID}'")


def check_duplicates(client: bigquery.Client, df: pd.DataFrame) -> pd.DataFrame:
    """
    Kiá»ƒm tra trÃ¹ng láº·p á»Ÿ cáº¥p tá»«ng dÃ²ng (row-level).
    Sá»­ dá»¥ng 2-stage approach:
      Stage 1: Lá»c theo ma_bn (mÃ£ bá»‡nh nhÃ¢n) â†’ tÃ¬m táº¥t cáº£ lÆ°á»£t KCB cá»§a cÃ¹ng BN
      Stage 2: Merge chÃ­nh xÃ¡c theo composite key (ma_cskcb, ma_bn, ma_loaikcb, ngay_vao, ngay_ra)
    Tráº£ vá» DataFrame chá»©a cÃ¡c dÃ²ng trÃ¹ng (tá»« df gá»‘c), hoáº·c DataFrame rá»—ng.
    """
    try:
        client.get_table(FULL_TABLE_ID)
    except NotFound:
        return pd.DataFrame()  # Table chÆ°a tá»“n táº¡i â†’ khÃ´ng trÃ¹ng

    # â”€â”€ Stage 1: Lá»c theo mÃ£ bá»‡nh nhÃ¢n â”€â”€
    ma_bn_list = df["ma_bn"].dropna().unique().tolist()
    if not ma_bn_list:
        return pd.DataFrame()

    # Chia batch náº¿u danh sÃ¡ch BN quÃ¡ lá»›n (BigQuery giá»›i háº¡n query size)
    BATCH_SIZE = 5000
    key_cols_sql = ", ".join(ROW_KEY_COLS)
    all_bq_rows = []

    for i in range(0, len(ma_bn_list), BATCH_SIZE):
        batch = ma_bn_list[i:i + BATCH_SIZE]
        ma_bn_in = ", ".join([f"'{str(m)}'" for m in batch])
        query = f"""
            SELECT {key_cols_sql}
            FROM `{FULL_TABLE_ID}`
            WHERE ma_bn IN ({ma_bn_in})
        """
        batch_num = i // BATCH_SIZE + 1
        total_batches = (len(ma_bn_list) + BATCH_SIZE - 1) // BATCH_SIZE
        if total_batches > 1:
            print(f"  â³ Äang truy váº¥n BigQuery (batch {batch_num}/{total_batches})...")
        else:
            print("  â³ Äang truy váº¥n BigQuery Ä‘á»ƒ so sÃ¡nh...")
        result = client.query(query).to_dataframe()
        if not result.empty:
            all_bq_rows.append(result)

    if not all_bq_rows:
        return pd.DataFrame()

    bq_rows = pd.concat(all_bq_rows, ignore_index=True)

    if bq_rows.empty:
        return pd.DataFrame()

    # â”€â”€ Stage 2: Merge chÃ­nh xÃ¡c theo composite key â”€â”€
    # Chuáº©n hÃ³a kiá»ƒu dá»¯ liá»‡u Ä‘á»ƒ merge chÃ­nh xÃ¡c
    merge_df = df[ROW_KEY_COLS].copy()
    for col in ["ma_cskcb", "ma_bn"]:
        merge_df[col] = merge_df[col].astype(str)
        bq_rows[col] = bq_rows[col].astype(str)
    for col in ["ma_loaikcb"]:
        merge_df[col] = pd.to_numeric(merge_df[col], errors="coerce")
        bq_rows[col] = pd.to_numeric(bq_rows[col], errors="coerce")
    for col in ["ngay_vao", "ngay_ra"]:
        merge_df[col] = pd.to_datetime(merge_df[col], errors="coerce")
        bq_rows[col] = pd.to_datetime(bq_rows[col], errors="coerce")

    # ÄÃ¡nh dáº¥u dÃ²ng nÃ o trÃ¹ng báº±ng merge indicator
    merged = merge_df.merge(bq_rows, on=ROW_KEY_COLS, how="inner")

    if merged.empty:
        return pd.DataFrame()

    # Tráº£ vá» index cá»§a cÃ¡c dÃ²ng trÃ¹ng trong df gá»‘c
    # Merge láº¡i vá»›i df Ä‘á»ƒ láº¥y Ä‘Ãºng index
    dup_mask = df[ROW_KEY_COLS].apply(tuple, axis=1).isin(
        merged[ROW_KEY_COLS].apply(tuple, axis=1)
    )
    return df[dup_mask]


def upload_data(client: bigquery.Client, df: pd.DataFrame):
    """Upload DataFrame lÃªn BigQuery."""
    job_config = bigquery.LoadJobConfig(
        schema=SCHEMA,
        write_disposition=bigquery.WriteDisposition.WRITE_APPEND,
    )

    print(f"  â³ Äang upload {len(df)} dÃ²ng lÃªn {FULL_TABLE_ID}...")
    job = client.load_table_from_dataframe(df, FULL_TABLE_ID, job_config=job_config)
    job.result()  # Wait for completion

    table = client.get_table(FULL_TABLE_ID)
    print(f"  âœ… Upload thÃ nh cÃ´ng! Tá»•ng sá»‘ dÃ²ng trÃªn BigQuery: {table.num_rows}")


# â”€â”€â”€ Main â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def main():
    if len(sys.argv) < 2:
        print("âŒ CÃ¡ch dÃ¹ng: python upload_to_bigquery.py <Ä‘Æ°á»ng_dáº«n_file_excel>")
        print("   VÃ­ dá»¥: python upload_to_bigquery.py CPBQ.xlsx")
        sys.exit(1)

    filepath = sys.argv[1]
    if not os.path.exists(filepath):
        print(f"âŒ KhÃ´ng tÃ¬m tháº¥y file: {filepath}")
        sys.exit(1)

    filename = os.path.basename(filepath)
    print(f"\n{'='*60}")
    print(f"ğŸ“Š UPLOAD Dá»® LIá»†U BHYT LÃŠN BIGQUERY")
    print(f"{'='*60}")
    print(f"  ğŸ“ File: {filename}")
    print(f"  ğŸ¯ Target: {FULL_TABLE_ID}")
    print(f"  ğŸ“ Location: {LOCATION}")
    print()

    # â”€â”€ Step 1: Read Excel â”€â”€
    print("ğŸ“– BÆ°á»›c 1: Äá»c file Excel...")
    try:
        df = pd.read_excel(filepath, sheet_name=SHEET_NAME, engine="openpyxl")
        print(f"  âœ… Äá»c Ä‘Æ°á»£c {len(df)} dÃ²ng, {len(df.columns)} cá»™t tá»« sheet '{SHEET_NAME}'")
    except Exception as e:
        print(f"  âŒ Lá»—i Ä‘á»c file: {e}")
        sys.exit(1)

    # â”€â”€ Step 2: Transform data â”€â”€
    print("\nğŸ”„ BÆ°á»›c 2: Chuáº©n hÃ³a dá»¯ liá»‡u...")
    df = transform_dataframe(df, filename)

    # Show summary
    combos = df[["nam_qt", "thang_qt", "ma_cskcb"]].drop_duplicates()
    print(f"\n  ğŸ“‹ TÃ³m táº¯t dá»¯ liá»‡u:")
    for _, row in combos.iterrows():
        subset = df[(df["nam_qt"] == row["nam_qt"]) &
                     (df["thang_qt"] == row["thang_qt"]) &
                     (df["ma_cskcb"] == row["ma_cskcb"])]
        print(f"     - {int(row['thang_qt']):02d}/{int(row['nam_qt'])} | "
              f"CSKCB: {row['ma_cskcb']} | "
              f"{len(subset)} dÃ²ng | "
              f"Tá»•ng chi: {subset['t_tongchi'].sum():,.0f} VND")

    # â”€â”€ Step 3: Connect to BigQuery â”€â”€
    print("\nğŸ”— BÆ°á»›c 3: Káº¿t ná»‘i BigQuery...")
    try:
        creds = get_credentials()
        client = bigquery.Client(project=PROJECT_ID, location=LOCATION, credentials=creds)
        print(f"  âœ… ÄÃ£ káº¿t ná»‘i project '{PROJECT_ID}'")
    except Exception as e:
        print(f"  âŒ Lá»—i káº¿t ná»‘i: {e}")
        print("  ğŸ’¡ Kiá»ƒm tra file credentials/client_secret.json")
        sys.exit(1)

    # â”€â”€ Step 4: Ensure dataset & table â”€â”€
    print("\nğŸ“¦ BÆ°á»›c 4: Kiá»ƒm tra dataset & table...")
    ensure_dataset(client)
    ensure_table(client)

    # â”€â”€ Step 5: Check duplicates â”€â”€
    print("\nğŸ” BÆ°á»›c 5: Kiá»ƒm tra trÃ¹ng láº·p (row-level)...")
    print(f"  ğŸ”‘ Composite key: {' + '.join(ROW_KEY_COLS)}")
    dup_df = check_duplicates(client, df)

    if not dup_df.empty:
        # Thá»‘ng kÃª trÃ¹ng theo thÃ¡ng/CSKCB
        dup_summary = dup_df.groupby(["nam_qt", "thang_qt", "ma_cskcb"]).size().reset_index(name="so_dong")
        print(f"  âš ï¸  PhÃ¡t hiá»‡n {len(dup_df)}/{len(df)} dÃ²ng Ä‘Ã£ tá»“n táº¡i trÃªn BigQuery:")
        for _, row in dup_summary.iterrows():
            print(f"     - {int(row['thang_qt']):02d}/{int(row['nam_qt'])} | "
                  f"CSKCB: {row['ma_cskcb']} | {row['so_dong']} dÃ²ng trÃ¹ng")

        new_count = len(df) - len(dup_df)
        print(f"  â„¹ï¸  DÃ²ng má»›i (chÆ°a cÃ³ trÃªn BQ): {new_count}")

        choice = input("\n  Báº¡n muá»‘n:\n"
                       "    [1] Bá» qua pháº§n trÃ¹ng, chá»‰ upload pháº§n má»›i\n"
                       "    [2] Upload táº¥t cáº£ (cho phÃ©p trÃ¹ng)\n"
                       "    [3] XÃ³a dá»¯ liá»‡u trÃ¹ng cÅ© rá»“i upload láº¡i táº¥t cáº£\n"
                       "    [0] Há»§y\n"
                       "  Chá»n (0/1/2/3): ").strip()

        if choice == "0":
            print("\n  âŒ ÄÃ£ há»§y upload.")
            sys.exit(0)
        elif choice == "1":
            # Lá»c chÃ­nh xÃ¡c tá»«ng dÃ²ng trÃ¹ng, giá»¯ láº¡i dÃ²ng má»›i
            dup_keys = set(dup_df[ROW_KEY_COLS].apply(tuple, axis=1))
            df = df[~df[ROW_KEY_COLS].apply(tuple, axis=1).isin(dup_keys)]
            if len(df) == 0:
                print("\n  â„¹ï¸  KhÃ´ng cÃ²n dá»¯ liá»‡u má»›i Ä‘á»ƒ upload.")
                sys.exit(0)
            print(f"\n  â„¹ï¸  CÃ²n láº¡i {len(df)} dÃ²ng má»›i Ä‘á»ƒ upload.")
        elif choice == "3":
            # XÃ³a chÃ­nh xÃ¡c tá»«ng nhÃ³m dÃ²ng trÃ¹ng trÃªn BigQuery
            dup_groups = dup_df.groupby(["nam_qt", "thang_qt", "ma_cskcb"])
            for (nam, thang, cskcb), group in dup_groups:
                # Build conditions cho tá»«ng dÃ²ng trÃ¹ng trong nhÃ³m
                row_conditions = []
                for _, r in group.iterrows():
                    ngay_vao_str = r["ngay_vao"].strftime("%Y-%m-%d %H:%M:%S") if pd.notna(r["ngay_vao"]) else None
                    ngay_ra_str = r["ngay_ra"].strftime("%Y-%m-%d %H:%M:%S") if pd.notna(r["ngay_ra"]) else None
                    parts = [f"ma_cskcb = '{r['ma_cskcb']}'",
                             f"ma_bn = '{r['ma_bn']}'"]
                    parts.append(f"ma_loaikcb = {int(r['ma_loaikcb'])}" if pd.notna(r["ma_loaikcb"]) else "ma_loaikcb IS NULL")
                    parts.append(f"ngay_vao = '{ngay_vao_str}'" if ngay_vao_str else "ngay_vao IS NULL")
                    parts.append(f"ngay_ra = '{ngay_ra_str}'" if ngay_ra_str else "ngay_ra IS NULL")
                    row_conditions.append(f"({' AND '.join(parts)})")

                # XÃ³a theo batch má»—i nhÃ³m thÃ¡ng/CSKCB
                delete_query = f"""
                    DELETE FROM `{FULL_TABLE_ID}`
                    WHERE nam_qt = {int(nam)} AND thang_qt = {int(thang)}
                      AND ({' OR '.join(row_conditions)})
                """
                client.query(delete_query).result()
                print(f"  ğŸ—‘ï¸  ÄÃ£ xÃ³a {len(group)} dÃ²ng cÅ©: {int(thang):02d}/{int(nam)} | CSKCB: {cskcb}")
        # choice == "2": upload all (do nothing)
    else:
        print("  âœ… KhÃ´ng phÃ¡t hiá»‡n trÃ¹ng láº·p.")

    # â”€â”€ Step 6: Upload â”€â”€
    print(f"\nğŸš€ BÆ°á»›c 6: Upload dá»¯ liá»‡u...")
    upload_data(client, df)

    print(f"\n{'='*60}")
    print(f"ğŸ‰ HOÃ€N THÃ€NH!")
    print(f"{'='*60}")
    print(f"  Äá»ƒ truy váº¥n dá»¯ liá»‡u, vÃ o: https://console.cloud.google.com/bigquery?project={PROJECT_ID}")
    print()


if __name__ == "__main__":
    main()
